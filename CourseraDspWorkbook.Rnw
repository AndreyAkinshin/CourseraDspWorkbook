\documentclass{report}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{indentfirst}

\usepackage{color}
\usepackage{colortbl}
\usepackage[linktocpage=true,plainpages=false,pdfpagelabels=false]{hyperref}
\definecolor{linkcolor}{rgb}{0.9,0,0}
\definecolor{citecolor}{rgb}{0,0.6,0}
\definecolor{urlcolor}{rgb}{0,0,1}
\hypersetup{
	colorlinks, linkcolor={linkcolor},
	citecolor={citecolor}, urlcolor={urlcolor}
}
\sloppy
\clubpenalty=10000
\widowpenalty=10000   

\renewcommand{\chaptername}{Module}
\makeatletter
\renewcommand{\@chapapp}{Module}
\makeatother

\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figure/dsp-', fig.align='center', cache = T, fig.width=7, fig.height=3)
options(formatR.arrow=TRUE,width=90)
@

\title{Digital signal processing workbook}		
\author{Andrey Akinshin}
\date{}
\maketitle

\tableofcontents
\clearpage

\chapter{Introduction}

This document is a workbook for the ``Digital signal processing'' Coursera course (\url{https://www.coursera.org/course/dsp}). It uses the R language and the knitr package for calculation and plot rendering.

Help code:

<<base-functions, cache=FALSE, echo=TRUE>>=
library("ggplot2")
library("grid")
library("gridExtra")
library("phonTools")
# Discrete signal plot
dsplot <- function(x, y, xlab="", ylab="") {
  d <- data.frame(x = x, y = y, y0 = rep(0, length(y)))
  p <- ggplot() +
    geom_segment(data=d, mapping=aes(x=x, y=y0, xend=x, yend=y), size=1, color="red") +
    geom_point(data=d, mapping=aes(x=x, y=y), size=2, shape=21, fill="red") +
    scale_x_continuous(name=xlab) + 
    scale_y_continuous(name=ylab)
  p
}
# Continuous signal plot
csplot <- function(x, y, xlab="", ylab="") {
  d <- data.frame(x = x, y = y)
  p <- ggplot(data=d, aes(x=x, y=y)) +
    geom_line(color="red") +
    scale_x_continuous(name=xlab) + 
    scale_y_continuous(name=ylab)
  p
}
# dft plot
dftplot <- function(x, xlab="", onlyhalf=F) {
  n <- 1:length(x)
  if (onlyhalf)
    n <- 1:(length(x)/2)
  X <- fft(x)
  grid.arrange(dsplot(n, round(Re(X),4),  ylab="Re(X[n])", xlab=xlab), 
               dsplot(n, round(Im(X),4),  ylab="Im(X[n])", xlab=xlab), ncol=2)
  grid.arrange(dsplot(n, round(Mod(X),4), ylab="Mod(X[n])", xlab=xlab), 
               dsplot(n, round(Arg(X),4), ylab="Arg(X[n])", xlab=xlab), ncol=2)
}
# Discrete-Time Fourier Transform
dtft <- function(x) {
  func <- function(w) {
    sum(x * exp(-1i * w * (1:length(x))))
  }
  Vectorize(func)
}
# Discrete-Time Fourier Transform plot
dtftplot <- function(x, xlab="", ylab="") {
  func <- dtft(x)
  t <- seq(-pi, pi, by=0.01)
  value <- func(t)
  grid.arrange(csplot(t, round(Re(value),4),  ylab="Re(X[n])",  xlab=xlab), 
               csplot(t, round(Im(value),4),  ylab="Im(X[n])",  xlab=xlab), ncol=2)
  grid.arrange(csplot(t, round(Mod(value),4), ylab="Mod(X[n])", xlab=xlab), 
               csplot(t, round(Arg(value),4), ylab="Arg(X[n])", xlab=xlab), ncol=2)
}
@

\chapter{Discrete-time (DT) signals}
\newpage

\section{Discrete-time signals}

\subsection{The delta signal}
$$ x[n] = \delta[n] $$
<<m2.1-delta-signal, echo=TRUE>>=
n <- -16:16
x <- ifelse(n == 0, 1, 0)
dsplot(n, x)	
@

\subsection{The unit step}
$$ x[n] = 1 $$
<<m2.1-unit-step, echo=TRUE>>=
n <- -16:16
x <- ifelse(n >= 0, 1, 0)
dsplot(n, x)	
@

\subsection{The exponential decay}
$$ x[n] = |a|^n u[n], \quad |a| < 1 $$
<<m2.1-exponential-decay, echo=TRUE>>=
a <- 0.5
n <- -16:16
x <- ifelse(n >= 0, abs(a)^n, 0)
dsplot(n, x)	
@

\subsection{The sinusoid}
$$ x[n] = sin(\omega_0 n + \theta) $$
<<m2.1-sinusoid, echo=TRUE>>=
omega0 <- pi/10; theta <- pi/2
n <- -16:16
x <- sin(omega0 * n + theta)
dsplot(n, x)	
@

\subsection{Shift of a finite-length: finite-support}
$$ \overline x[n] = x[n-z] $$
<<m2.1-shift-finite, echo=TRUE>>=
shift <- function(x, z) c(rep(0, z), x[1:(length(x)-z)])
n <- 0:7
x1 <- 1-n*0.1
x2 <- shift(x1, 1)
x3 <- shift(x1, 2)
x4 <- shift(x1, 3)
grid.arrange(dsplot(n, x1), dsplot(n, x2), ncol=2)
grid.arrange(dsplot(n, x3), dsplot(n, x4), ncol=2)
@
\newpage

\subsection{Shift of a finite-length: periodic extension}
$$ \tilde x[n] = x[n-z] $$
<<m2.1-shift-periodic, echo=TRUE>>=
shift <- function(x, z) c(x[(length(x)-z+1):length(x)], x[1:(length(x)-z)])
n <- 0:7
x1 <- 1-n*0.1
x2 <- shift(x1, 1)
x3 <- shift(x1, 2)
x4 <- shift(x1, 3)
grid.arrange(dsplot(n, x1), dsplot(n, x2), ncol=2)
grid.arrange(dsplot(n, x3), dsplot(n, x4), ncol=2)
@
\newpage

\subsection{Energy and power}

General formulas:

$$ E_x = \sum_{n=-\infty}^{\infty} |x[n]|^2$$

$$ P_x = \lim_{N \to \infty} \frac{1}{2N+1} $$

Periodic signals:

$$ E_{\tilde x} = \infty $$

$$ P_{\tilde x} = \sum_{n = 0}^{N-1} |\tilde x [n]|^2 $$
\newpage

\section{The complex exponential}

$$
x[n] = 
A e^{j(\omega n + \phi)} =
A \big( \cos(\omega n + \phi) + j \sin(\omega n + \phi) \big)
$$

$$
cos(\omega n + \phi) = a \cos(\omega n) + b \sin (\omega n), \quad a = \cos \phi, \; b = \sin \phi
$$

$$
Re \{ e^{j(\omega n + \phi)} \} = Re \{ e^{j \omega n} e^{j\phi} \}
$$

$$
e^{j\alpha} = \cos \alpha + j \sin \alpha
$$

$$
\textrm{Rotation:} \quad z' = z e^{j \alpha}
$$

$$
\textrm{Signal:} \quad x[n] = e^{j \omega n}; \quad x[n + 1] = e^{j\omega} x[n]
$$

$$
\textrm{Signal:} \quad x[n] = e^{j \omega n + \phi}; \quad x[n + 1] = e^{j\omega} x[n], \quad x[0] = e^{j \phi}
$$

$$
e^{j\omega n} \; \textrm{peridoic} \Leftrightarrow \omega = \frac{M}{N} 2 \pi, \; M,N \in \mathbb{N}
$$

$$
e^{j \omega} = e^j(\omega + 2 k \pi), \forall k \in \mathbb{N}
$$

\newpage

\section{The Karplus-Strong Algorithm}

Main equation:

$$ y[n] = \alpha y[n - M] + x[n] $$

<<m2.3-karplus-strong, echo=TRUE>>=
ks <- function(x, M, alpha, N) {
  y <- c()
  for (n in 1:N) {
    yn <- alpha * ifelse(n > M, y[n-M], 0) + 
          ifelse(n <= length(x), x[n], 0)
    y <- c(y, yn)
  }
  y
}
@

\subsection{Sine wave}

$M = 100, \alpha = 1, \overline x[n] = \sin (2\pi n /100)$ for $0 \leq n < 100$ and zero elsewhere

<<m2.3-sin-wave, echo=TRUE>>=
M <- 100; alpha <- 1; n <- 0:99
x <- sin(2*pi*n/100)
y <- ks(x, M, alpha, 1000)
grid.arrange(dsplot(n, x), csplot(1:length(y), y), ncol=2)
@

\subsection{Proto-violin}

$M = 100, \alpha = 0.95, \overline x[n]: $ zero-mean sawtooth wave between 0 and 99, zero elsewhere

<<m2.3-proto-violin, echo=TRUE>>=
M <- 100; alpha <- 0.95; n <- 0:99
x <- -1 + 2*n/100
y <- ks(x, M, alpha, 1000)
grid.arrange(dsplot(n, x), csplot(1:length(y), y), ncol=2)
@

\subsection{Random}

$M = 100, \alpha = 0.9, \overline x[n]: $ 100 random values between 0 and 99, zero elsewhere

<<m2.3-random, echo=TRUE>>=
M <- 100; alpha <- 0.9; n <- 0:99
x <- runif(100, -1, 1)
y <- ks(x, M, alpha, 1000)
grid.arrange(dsplot(n, x), csplot(1:length(y), y), ncol=2)
@

\newpage

\chapter{Euclid and Hilbert}

\section{From Euclid to Hilbert}

\subsection{Space of square-integrable functions over $[-1,1]: l_2([-1,1])$}

<<m3.1-vectors-as-general-objects, echo=TRUE>>=
t <- seq(-1, 1, by=0.01)
f1 <- 2*pi; x1 <- sin(f1 * t)
f2 <- 5*pi; x2 <- sin(f2 * t)
grid.arrange(csplot(t, x1), csplot(t, x2), ncol=2)
@

$$ x^{(1)} = \sin(f_1 t), \quad f_1 = 2\pi $$

$$ x^{(2)} = \sin(f_2 t), \quad f_2 = 5\pi $$

$$ \big< x^{(1)}, x^{(2)} \big> = \int_{-1}^{1} \sin (f_1 t) \sin(f_2 t) dt $$

$x^{(1)} \bot x^{)(2)}$ if $f_1 \neq f_2$ and $f_1, f_2$ integer multiples of a fundamental (harmonically related)

$$ \sin(f_1 t) \sin(f_2 t), \quad f_1 = 2\pi, \; f_2 = 5\pi $$

<<m3.1-orthogonality-in-a-functional-vector-space, echo=TRUE>>=
csplot(t, x1*x2)
round(integrate(function(x) sin(f1*x)*sin(f2*x), lower=-1, upper=1)$value, 10)
@

\subsection{Interesting question when the space has $\infty$ dimensions}

$$ \sun_{k=0}^{N} x^{(2k+1)}, \quad x^{(n)} = \sin(\pi n t)/n, t \in [-1, 1] $$

<<m3.1-infty-space, echo=TRUE>>=
inftySpace <- function(N) {
  t <- seq(-1, 1, by=0.001)
  x <- rep(0, length(t))
  for (k in 0:N) {
    n <- 2 * k + 1
    x <- x + sin(pi*n*t)/n
  }
  csplot(t, x)
}
inftySpace(0)
inftySpace(1)
inftySpace(2)
inftySpace(10)
inftySpace(50)
inftySpace(150)
@

\newpage

\section{Hilbert Space, properties and bases}

\subsection{Formal properties of a vector space}

For $x, y, z \in V$ and $\alpha, \beta \in \mathbb{C}$:

\begin{itemize}
  \item $x + y = y + x$
  \item $(x + y) + z = x + (y + z)$
  \item $\alpha (x + y) = \alph x + \alpha y$
  \item $(\alpha + \beta)x = \alpha x + \beta x$
  \item $\alpha(\beta x) = (\alpha \beta) x$
  \item $\exists 0 \in V \quad | \quad x + 0 = 0 + x = x$
  \item $\forall x \in V \; \exists(-x) \quad | \quad x + (-x) = 0$
\end{itemize}

\subsection{Formal properties of the inner product}

For $x, y, z \in V$ and $\alpha \in \mathbb{C}$:

\begin{itemize}
  \item $\langle x + y, z \rangle = \langle x,z \rangle + \langle y,z \rangle$
  \item $\langle x, y \rangle = \langle y,x \rangle^*$
  \item $\langle \alpha x, y \rangle = \alpha^* \langle x,y \rangle$
  \item $\langle x, \alpha y \rangle = \alpha \langle x, y \rangle$
  \item $\langle x, x \rangle \geq 0$
  \item $\langle x, x \rangle = 0 \Leftrightarrow x = 0$
  \item if $\langle x, y \rangle = 0$ and $x, y \neq 0$ then $x$ and $y$ are called orthogonal
\end{itemize}

\subsection{Inner product for signals}

$$ \langle x, y \rangle = \sum_{n=0}^{N-1} x^*[n]y[n]$$

$$ \langle x, y \rangle = \sum_{n=-\infty}^{\infty} x^*[n]y[n]$$

We require sequences to be \textit{square-summable}: $\sum |x[n]|^2 < \infty$.

Space of square-summable sequences: $l_2(\mathbb{Z})$.

\subsection{Special bases}

Orthogonal basis: $ \langle w^(k), w^{(n)} \rangle = 0 $ for $k \neq n$.

Orthonormal basis: $ \langle w^(k), w^{(n)} \rangle = \sigma[n-k] $.

\newpage

\section{Hilbert Space and approximation}

\subsection{Parseval's Theorem}

$$ x = \sum_{k=0}^{K-1} \alpha_k w^{(k)} $$

For an orthonormal basis:

$$
 || x ||^2 = \sum_{k=0}^{K-1} |\alpha_k|^2
$$

\subsection{Gram-Schmidt algorithm}

The Gram-Schmidt algorithm leads to an orthonormal basis for $P_N([-1,1])$:

$$
\begin{array}{c}
  u^{(0)} = \sqrt{1/2} \\
  u^{(1)} = \sqrt{3/2}t \\
  u^{(2)} = \sqrt{5/8} (3t^2-1) \\
  u^{(3)} = \ldots
\end{array}
$$

$$
  \alpha_k = \langle u^{(k)}, x \rangle
$$

\chapter{Fourier Analysis}

\section{Exploration via a change of basis}

\subsection{The Fourier Basis for $\mathbb{C}^N$}

In vector notation:

$$ \{ w^{(k)}_{k=0,1,\ldots,N-1} \} $$

with

$$ w_n^{k} = e^{j \frac{2\pi}{N} nk} $$

is an orthogonal basis in $\mathbb{C}^N$.

<<m4.1-basis-vectors, echo=TRUE>>=
N <- 64
plotW <- function(k) {
  n <- 0:(N-1)
  w <- exp(1i * 2*pi/N * n * k)
  grid.arrange(dsplot(n, Re(w), xlab=paste0("k=", k), ylab="Re"), 
               dsplot(n, round(Im(w),5), xlab=paste0("k=", k), ylab="Im"), ncol=2)
}
for (k in c(0,1,2,3,4,5,16,20,30,31,32,33,60,62,63))
  plotW(k)
@

\section{The Discrete Fourier Transform}

Analysis formula:

$$
  X[k] = sum_{n=0}^{N=1} x[n] e^{-j\frac{2\pi}{N}nk}, \quad k = 0,1,\ldots,N-1
$$

N-point signal in the \textit{frequency domain}.

Synthesis formula:

$$
  x[n] = \frac{1}{N} sum_{k=0}^{N-1} X[k] e^{j\frac{2\pi}{N}nk}, \quad n = 0,1,\ldots,N-1
$$

N-point signal in the \textit{``time'' domain}.

DFT is obviously linear:

$$
  DFT \{ \alpha x[n] + \beta y[n] \} = \alpha DFT \{ x[n] \} + \beta DFT \{ y[n] \}
$$

\subsection{DFT of the delta signal}

$$
  DFT of x[n] = \sigma[n], \quad x[n] \in \mathbb{C}^N
$$

$$
  X[k] = 1
$$

<<m4.2-delta-signal, echo=TRUE>>=
n <- 0:15
x <- c(1, rep(0,15))
X <- fft(x)
grid.arrange(dsplot(n, x, ylab="x[n]"), dsplot(n, Re(X), ylab="X[n]"), ncol=2)
@

\subsection{DFT of the uniform signal}

$$
DFT of x[n] = 1, \quad x[n] \in \mathbb{C}^N
$$

$$
X[k] = N \sigma[k]
$$

<<m4.2-uniform-signal, echo=TRUE>>=
n <- 0:15
x <- rep(1, 16)
X <- fft(x)
grid.arrange(dsplot(n, x, ylab="x[n]"), dsplot(n, Re(X), ylab="X[n]"), ncol=2)
@

\subsection{DFT of the cos signal}

$$
DFT of x[n] = 3\cos(2\pi/16 n), \quad x[n] \in \mathbb{C}^64
$$

$$
X[k] = \left\{
  \begin{array}{ll}
     96 & \textrm{for} k = 4, 60 \\
     0 & otherwise
  \end{array}
\right.
$$

<<m4.2-cos-signal, echo=TRUE>>=
n <- 0:63
x <- 3 * cos(2*pi/16*n)
X <- fft(x)
grid.arrange(dsplot(n, Re(X), ylab="Re(X[n])"), 
             dsplot(n, round(Im(X),4), ylab="Im(X[n])"), ncol=2)
@

$$
DFT of x[n] = 3\cos(2\pi/16 n + \pi/3), \quad x[n] \in \mathbb{C}^64
$$

$$
X[k] = \left\{
  \begin{array}{ll}
    96 e^{j \frac{\pi}{3}} & \textrm{for} k = 4 \\
    96 e^{-j \frac{\pi}{3}} & \textrm{for} k = 60 \\
    0 & otherwise
  \end{array}
\right.
$$

<<m4.2-cos-signal2, echo=TRUE>>=
n <- 0:63
x <- 3 * cos(2*pi/16*n + pi/3)
X <- fft(x)
grid.arrange(dsplot(n, Re(X), ylab="Re(X[n])"), 
             dsplot(n, round(Im(X),4), ylab="Im(X[n])"), ncol=2)
@

$$
DFT of x[n] = 3\cos(2\pi/10 n), \quad x[n] \in \mathbb{C}^64
$$

<<m4.2-cos-signal3, echo=TRUE>>=
n <- 0:63
x <- 3 * cos(2*pi/10*n)
X <- fft(x)
grid.arrange(dsplot(n, Mod(X), ylab="Mod(X[n])"), 
             dsplot(n, Arg(X), ylab="Arg(X[n])"), ncol=2)
@

\subsection{DFT of length-M step in $\mathbb{C}^N$}

$$
  x[n] = \sum_{h=0}^{M-1} \sigma[n-h], \quad n = 0, 1, \ldots, N-1
$$

$$
  X[k] = \dfrac{\sin(\frac{\pi}{N} Mk)}{\sin(\frac{\pi}{N}k)} e^{-j\frac{pi}{N}(M-1)k}
$$

<<m4.2-step, echo=TRUE>>=
plotStep <- function(M) {
  n <- 0:63
  x <- c(rep(1, M), rep(0, 64-M))
  X <- fft(x)
  grid.arrange(dsplot(n, Mod(X), ylab="Mod(X[n])", xlab="M=4"), 
               dsplot(n, Arg(X), ylab="Arg(X[n])", xlab="M=4"), ncol=2)
}
plotStep(4)
@

\section{The DFT in practice}

\subsection{Average Monthly Temperatures at Nottingham, 1920-1939}

<<m4.3-nottingham, echo=TRUE>>=
x <- as.integer(nottem)
n <- 1:length(x)
X <- fft(x) / length(x)
csplot(n, x, ylab="x (Temperature)")
grid.arrange(dsplot(n, Re(X), ylab="Re(X[n])"), 
             dsplot(n, Im(X), ylab="Im(X[n])"), ncol=2)
MainPeak = which.max(abs(Re(X[-1])))
cat("Average value:", Re(X[1]))
cat("DFT main peak for k = ", MainPeak, ", value = ", abs(Re(X[MainPeak+1])))
cat("Month count:", length(n))
cat("Frequency: ", length(n) , "/", MainPeak, "=", length(n)/MainPeak, "months")
cat("Temperature excursion: ", Re(X[1]), "+-", 2 * abs(Re(X[MainPeak+1])))
@

\section{DFT, DFS, DTFT}

\subsection{Discrete-Time Fourier Transform (DTFT)}

Formal definition:

\begin{itemize}
  \item $x[n] \in l_2(\mathbb{Z})$
  \item define the \textit{function} of $\omega \in \mathbb{R}$
  $$ F(\omega) = \sum_{n=-\infty}^{\infty} x[n] e^{-j \omega n} $$
  \item inversion (when $F(\omega)$ exists):
  $$ x[n] = \frac{1}{2\pi} \int_{-pi}^{pi} F(\omega) e^{j \omega n} d\omega$$
  \item F(\omega) is $2\pi$-periodic
  \item to stress periodicity (and for other reasons) we will write
  $$ X(e^{j\omega}) = \sum_{n=-\infty}^{\infty} x[n] e^{-j \omega n} $$
  \item by convention, $X(e^{j \omega})$ is represented over $[-pi; pi]$
\end{itemize}

<<m4.4-decay, echo=TRUE>>=
alpha <- 0.9; n <- 0:40
x <- alpha^n
dsplot(n, x)
dtftplot(x)
@
\newpage

\section{DTFT: intuition and properties}

\subsection{DTFT properties}
\begin{itemize}
  \item Linearity
    $$ DTFT \{ \alpha x[n] + \beta y[n] \} = \alpha X(e^{j\omega}) + \beta Y(e^{j\omega})$$
  \item Time shift
    $$ DTFT \{ x[n-M] \} = e^{-j\omega M} X(e^{j\omega}) $$
  \item Modulation (dual)
    $$ DTFT \{ e^{j \omega_0 n} x[n] \} = X(e^{j(\omega-\omega_0)}) $$
  \item Time reversal
    $$ DTFT \{ x[-n] \} = X(e^{-j\omega}) $$
  \item Conjugation
    $$ DTFT \{ x^*[n] \} = X^*(e^{-j\omega})$$
\end{itemize}

\subsection{Some particular cases}
\begin{itemize}
  \item if $x[n]$ is symmetric, the DTFT is symmetric:
    $$ x[n] = x[-n] \Leftrightarrow X(e^{j\omega}) = X(e^{-j\omega}) $$
  \item if $x[n]$ is real, the DTFT is Hermitian-symmetric:
    $$ x[n] = x^*[n] \Leftrightarrow X(e^{j\omega}) = X^*(e^{-j\omega})$$
  \item special case: if $x[n]$ is real, the magnitude of the DTFT is symmetric:
    $$ x[n] \in \mathbb{R} \Rightarrow |X(e^{j\omega})| = |X(e^{-j\omega})| $$
  \item more special case: if $x[n]$ is real and symmetric, $X(e^{j\omega})$ is also real and symmetric
\end{itemize}

\subsection{DTFT as basic expansion}

\begin{itemize}
  \item $ DFT \{ \delta[n] \} = 1 $
  \item $ DTFT \{ \delta[n] \} = 1 $
  \item $ DFT \{ 1 \} = N \delta[k] $
  \item $ DTFT \{ 1 \} = \tilde \delta(\omega) $
  \item $ DTFT \{ e^{j\omega_0 n} \} = \tilde \delta (\omega - \omega_0) $
  \item $ DTFT \{ \cos(\omega_0 n) \} = \Big( \tilde \delta (\omega - \omega_0) + \tilde \delta (\omega + \omega_0) \Big) / 2 $
  \item $ DTFT \{ \sin(\omega_0 n) \} = -j \Big( \tilde \delta (\omega - \omega_0) - \tilde \delta (\omega + \omega_0) \Big) / 2 $
\end{itemize}

\newpage

\section{Relationship between transforms}

\subsection{DTFT of periodic signals}

$$
  \tilde X(e^{j\omega}) = \frac{1}{N} \sum_{k=0}^{N-1} X[k] \tilde\delta(\omega - \frac{2\pi}{N} k)
$$

\subsection{32-tap sawtooth}

<<m4.6-sawtooth, echo=TRUE>>=
n <- 0:31
x <- -1 + 2*n/31
dsplot(n, x)
dftplot(x)
dtftplot(x)
@

\subsection{Interval indicator signal}

<<m4.6-indicator, echo=TRUE>>=
N <- 9
n <- 0:(N-1)
x <- rep(1, N)
dsplot(n, x)
dtftplot(x)
@


\section{Sinusoidal modulation}

\subsection{Classifying signals in frequency}

Three broad categories according to where most of the spectral energy resides:

\begin{itemize}
  \item lowpass signals (also known as ``baseband'' signals)
  \item highpass signals
  \item bandpass signals
\end{itemize}

\subsection{Sinusoidal modulation}

$$
  DTFT \{ x[n] \cos(\omega_c n) \} = 
  DTFT \Big\{ \frac{1}{2}e^{j \omega_c n}x[n] + \frac{1}{2} e^{-j\omega_c n} x[n] \Big\} =
  \frac{1}{2} \Big( X(e^{j(\omega-\omega_c)}) + X(e^{j(\omega+\omega_c))} \Big)
$$

\begin{itemize}
  \item usually $x[n]$ baseband
  \item $\omega_c$ is the \textit{carrier} frequency
\end{itemize}

Applications:

\begin{itemize}
  \item voice and music are lowpass signals
  \item radio channels are bandpass, in much higher frequencies
  \item modulation brings the baseband signal in the transmission band
  \item demodulation at the receiver brings it back
\end{itemize}

Demodulation in the frequency domain:

\begin{itemize}
  \item we recovered the baseband signal exactly...
  \item but we have some spurious high-frequency components
  \item in the next Module we will learn how to get rid of them!
\end{itemize}

\subsection{Tuning a guitar}

Problem (abstraction):
\begin{itemize}
  \item reference sinusoid at frequency $\omega_0$
  \item tunable sinusoid of frequency $\omega$
  \item make $\omega = \omega_0$ ``by ear''
\end{itemize}

The procedure:

\begin{enumerate}
  \item bring $\omega$ close to $\omega_0$ (easy)
  \item when $\omega \approx \omega_0$ play both sinusoid together
  \item trigonometry comes to the rescue:
    $$
      x[n] =
      cos(\omega_0 n) + cos(\omega n) =
      2 \cos(\frac{\omega_0+\omega}{2}n) \cos(\frac{\omega_0-\omega}{2}n) \approx
      2 \cos(\Delta_\omega n) \cos(\omega_0 n)
    $$
\end{enumerate}

Let's see what's happening:

$$ x[n] \approx 2 \cos(\Delta_\omega n) \cos(\omega_0 n) $$

\begin{itemize}
  \item ``error'' signal: $ 2 \cos(\Delta_\omega n) $
  \item modulation at $\omega_0$: $ \cos(\omega_0 n) $
  \item when $\omega \approx \omega_0$, the error signal is too low to be heard; modulation brings it up to hearing range and we perceive it as amplitude oscillations of the carrier frequency
\end{itemize}

Examples in the time domain :

$$
  \omega_0 = 2\pi \cdot 0.2,
  \omega = 2\pi \cdot 0.22
  \Delta_\omega = 2\pi \cdot 0.0100
$$

<<m4.7-tuning1, echo=TRUE>>=
omega0 <- 2*pi*0.2
omega <- 2*pi*0.22
t <- seq(0, 400, by=0.1)
x <- cos(omega0*t) + cos(omega*t)
csplot(t, x)
@

$$
\omega_0 = 2\pi \cdot 0.2,
\omega = 2\pi \cdot 0.21
\Delta_\omega = 2\pi \cdot 0.0050
$$

<<m4.7-tuning2, echo=TRUE>>=
omega0 <- 2*pi*0.2
omega <- 2*pi*0.21
t <- seq(0, 400, by=0.1)
x <- cos(omega0*t) + cos(omega*t)
csplot(t, x)
@

$$
\omega_0 = 2\pi \cdot 0.2,
\omega = 2\pi \cdot 0.205
\Delta_\omega = 2\pi \cdot 0.0025
$$

<<m4.7-tuning3, echo=TRUE>>=
omega0 <- 2*pi*0.2
omega <- 2*pi*0.205
t <- seq(0, 400, by=0.1)
x <- cos(omega0*t) + cos(omega*t)
csplot(t, x)
@

$$
\omega_0 = 2\pi \cdot 0.2,
\omega = 2\pi \cdot 0.201
\Delta_\omega = 2\pi \cdot 0.0005
$$

<<m4.7-tuning4, echo=TRUE>>=
omega0 <- 2*pi*0.2
omega <- 2*pi*0.201
t <- seq(0, 400, by=0.1)
x <- cos(omega0*t) + cos(omega*t)
csplot(t, x)
@

\section{The Short-Time Fourier Transform}

Dual-Tone Multi Frequency (DTMF) dialing:

<<m4.8-dtmf-freq, echo=TRUE>>=
f123 <- 697
f456 <- 770
f789 <- 852
fs0s <- 941
f147s <- 1209
f2580 <- 1336
f369s <- 1477
@

\subsection{Example: 1-5-9}

<<m4.8-dtmf-159, echo=TRUE>>=
step <- 0.1
t1 <- seq(0, 200, by=step)
t2 <- seq(0, 50, by=step)
x <- c(
  cos(t1 * f123) + cos(t1 * f147s), # 1
  t2 * 0,
  cos(t1 * f456) + cos(t1 * f2580), # 5
  t2 * 0,
  cos(t1 * f789) + cos(t1 * f369s), # 9
  t2 * 0)
N <- length(x)
t <- (1:N)*step
csplot(t, x)
X <- fft(x)
csplot(t[1:(N/2)], Mod(X)[1:(N/2)])
@

\subsection{Short-Time Fourier Transform}

Idea:

\begin{itemize}
  \item take small signal pieces of length L
  \item look at the DFT of each piece:
    $$ X[m; k] = \sum_{n=0}^{L-1} x[m+n] e^{-j\frac{2\pi}{L}nk} $$
\end{itemize}

<<m4.8-dtmf-stff, echo=TRUE>>=
count <- 6
L <- N / count
for (i in 1:count) {
  subt <- (L*(i - 1)+1):(L*i)
  X <- fft(x[subt])
  print(csplot(subt[1:(L/2)], Mod(X)[1:(L/2)]))
}
@

\subsection{Spectrogram}

Idea:

\begin{itemize}
  \item color-code the magnitude: dark is small, white is large
  \item use $10 \log_{10}(|X[m;k]|)$ to see better (pwer id dBs)
  \item plot spectral slices one after another
\end{itemize}

<<m4.8-dtmf-spectrogram, echo=TRUE>>=
spectrogram(x, fs=800, windowlength=N/60)
spectrogram(x, fs=800, windowlength=N/12)
@

\newpage

\section{FFT: history and algorithm}

\subsection{History}

\begin{itemize}
  \item Gauss computes trigonometric series efficiently in 1805
  \item Fourier invents Fourier series in 1807
  \item People start computing Fourier series, and develop tricks
  \item Good comes up with an algorithm in 1958
  \item Cooley and Tukey (re)-discover the fast Fourier transform algorithm in 1965 for N a power of a prime
  \item Winograd comines all methods to give the most efficient FFTs
\end{itemize}

\subsection{The DFT matrix}

\begin{itemize}
  \item $W_N = e^{-j\frac{2\pi}{N}}$ (or simply $W$ when $N$ is clear from the context)
  \item power of N can be taken modulo N, since $W^N = 1$.
  \item DFT Matrix of size $N$ by $N$:
  $$
    W = \begin{bmatrix}
     1 & 1 & 1 & 1 & \ldots & 1 \\
     1 & W^1 & W^2 & W^3 & \ldots & W^{N-1} \\
     1 & W^2 & W^4 & W^6 & \ldots & W^{2(N-1)} \\
     \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
     1 & W^{N-1} & W^{2(N-1)} & W^{3(N-1)} & \ldots & W^{(N-1)^2}
    \end{bmatrix}
  $$
\end{itemize}

\subsection{Divide and Conquer for DFT --- One step}

Recall: computing $X = W_n x has complexity O(N^2)$

Idea:

\begin{itemize}
  \item Take a problem of size N where N is a power of 2.
  \item Cut into two problems of size N/2 that use complexity $N^4/4$ each
  \item There might be some complexity to recover the full solution, say $N$.
  \item The divide-and-conquer solution has complexity $N^2/2+N$ for one step
  \item For $N \geq 4$ this is much better then $N^2$!
\end{itemize}

\subsection{Divide and Conquer for DFT --- Multiple steps}

Divide and conquer can be reapplied

\begin{itemize}
  \item If it worked once, it will work again (recall, $N=2^K$)
  \item the two problems of size $N/2$ into 4 problems of size $N/4$
  \item There might be some complexity to recover the full solution, say $N$ at each step
  \item You can do this $\log_2 (N-1)=K-1$ times, until problem of size 2 is obtained
  \item This requires order N complexity each time
  \item The divide-and-conquer solution has therefor complexity of order $N \log_2 N$
  \item For $N \geq 4$ this is much better then $N^2$!
\end{itemize}

\subsection{Conclusion}

Don't worry, be happy!

\begin{itemize}
  \item The Cooley-Tukey is the most popular algorithm, mostly for $N=2^N$
  \item Note that there is always a good FFT algorithm around the corner
  \item Do not zero-pad to lengthen a vector to have a size equal to a power of 2
  \item There are good packages out there (e.g. Fastest Fourier Transform in the West, SPIRAL)
  \item It does make a BIG difference!
\end{itemize}

\section{Why the DFT is useful: A few examples}

\subsection{Overview}

\begin{itemize}
  \item Analysis of musical instruments
  \item Approximation of periodical phenomena: the case of tides in Venice
  \item The secret of MP3 compression
  \item Magnetic Resonance Imaging
\end{itemize}

\newpage

\chapter{Linear Filters}

\section{Linear filters}

\subsection{A generic signal processing device}

The input signal: $x[n]$, the output signal: $y[n]$.

The device scheme:

$$
  x[n] \quad \longmapsto \quad \boxed{\mathcal{H}} \quad \longmapsto \quad y[n]
$$

The equation:

$$
  y[n] = \mathcal{H} \{ x[n] \}
$$

\subsection{Linearity}

$$
  \mathcal{H} \{ \alpha x_1[n] + \beta x_2[n] \} =
  \alpha \mathcal{H} \{ x_1[n] \} +
  \beta \mathcal{H} \{ x_2[n] \}
$$

\subsection{Time invariance}

$$
  y[n] = \mathcal{H} \{ x[n] \}
  \quad \Longleftrightarrow \quad 
  \mathcal{H} \{ x[n-n_0] \} = y[n-n_0]
$$

\subsection{Linear, time-invariant (LTI) systems}

$$
  y[n] = H(x[n], x[n-1], x[n-2], \ldots, y[n-1], y[n-2], \ldots)
$$

with $H(\cdot)$ a linear function of its arguments

\subsection{Impulse response}

$$
  h[n] = \mathcal{H} \{ \delta[n] \}
$$

Fundamental results: impulse response fully characterizes the LTI system!

\textbf{An example}

$$
h[n] = \alpha^n u[n]
$$

<<m5.1-lti-example1, echo=TRUE>>=
alpha <- 0.8
n <- -3:15
h <- c(0, 0, 0, alpha^(0:15))
dsplot(n, h, xlab="n", ylab="h[n]")
@

$$
x[n] = \begin{cases}
2 & n=0 \\
3 & n=1 \\
1 & n=2 \\
0 & otherwise
\end{cases}
$$

<<m5.1-lti-example2, echo=TRUE>>=
n <- -2:5
x <- c(0, 0, 2, 3, 1, 0, 0, 0)
dsplot(n, x, xlab="n", ylab="x[n]")
@

\begin{itemize}
  \item $x[n] = 2 \delta[n] + 3 \delta[n-1] + \delta[n-2]$
  \item we know the impulse response $h[n] = \mathcal{H} \{ \delta[n] \}$
  \item compute $y[n] = \mathcal{H} \{ x[n] \}$ exploiting linearity and time-invariance
\end{itemize}

$$
  \begin{array}{rl}
    y[n] = & \mathcal{H} \{ 2 \delta[n] + 3 \delta[n-1] + \delta[n-2] \} \\
         = & 2 \mathcal{H} \{ \delta[n] \} +
             3 \mathcal{H} \{ \delta[n-1] \} +
               \mathcal{H} \{ \delta[n-2] \} = \\
         = & 2h[n] + 3h[n-1] + h[n-2]
  \end{array}
$$

<<m5.1-lti-example3, echo=TRUE>>=
n <- -5:25
h <- c(0, 0, 0, 0, 0, alpha^(0:25))
shift <- function(s, z) c(rep(0, z), s[1:(length(s)-z)])
dsplot(n, 2*h, xlab="n", ylab="2h[n]")
dsplot(n, 3*shift(h,1), xlab="n", ylab="3h[n-1]")
dsplot(n, shift(h,2), xlab="n", ylab="h[n-2]")
y <- 2 * h + 3 * shift(h,1) + shift(h,2)
dsplot(n, y, xlab="n", ylab="y[n]")
@

\subsection{Convolution}

We can always write (remember Module 3.2):

$$
  x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]
$$

by linearity and time invariance:

$$
  y[n] = \sum_{k=-\infty}^{\infty} x[k] h[n-k] = x[n] \cdot h[n]
$$

\textbf{Performing the convolution algorithmically}

$$
  x[n] * h[n] = \sum_{k=-\infty}^{\infty} x[k] h[n-k]
$$

Ingredients:
\begin{itemize}
  \item a sequence $x[m]$
  \item a second sequence $h[m]$
\end{itemize}

The recipe:
\begin{itemize}
  \item time-reverse $h[m]$
  \item at each step $n$n (from $-\infty$ to $\infty$):
    \begin{itemize}
      \item center the time-reversed $h[m]$ in n (i.e. shift by $-n$)
      \item compute the inner product
    \end{itemize}
\end{itemize}

\subsection{Convolution properties}

\begin{itemize}
  \item linearity and time invariance (by definition)
  \item commutativity: $(x*h)[n] = (h*x)[n]$
  \item associativity for absolutely- and square-summable sequences: $((x*h)*w)[n]=(x*(h*w))[n]$
\end{itemize}

\section{Filtering by example}

\subsection{Donoising my Moving Average}
\begin{itemize}
  \item idea: replace each sample by the local average
  \item for instance: $y[n] = (x[n] + x[n-1])/2$
  \item more generally:
  $$
    y[n] = \frac{1}{M} \sum_{k=0}^{M-1} x[n-k]
  $$
\end{itemize}

\subsection{MA: impulse response}

$$
  h[n] = \frac{1}{M} \sum_{k=0}^{M-1} \delta[n-k] = \begin{cases}
    1/M & \textrm{for} \; 0 \leqslant n < M \\
    0 & \textrm{otherwise}
  \end{cases}
$$

\subsection{MA: analysis}
\begin{itemize}
  \item smoothing effect proportional to $M$
  \item number of operations and storage also proportional to $M$
\end{itemize}

\subsection{From the MA to a first-order recursion}
$$
  y_M[n] = \frac{1}{M} x[n] + \frac{1}{M} (
    x[n-1]+x[n-2]+\ldots+x[n-M+1]
  )
$$

where $(x[n-1]+x[n-2]+\ldots+x[n-M+1])$ is ``almost'' $y_{M-1}[n-1]$ i.e., moving average over $M-1$ points, delayed by one.

Formally:

$$
  y_M[n] = \frac{1}{M} \sum_{k=0}^{M-1} x[n-k]
$$

$$
  y_M[n-1] = \frac{1}{M} \sum_{k=1}^{M} x[n-k]
$$

$$
  y_{M-1}[n] = \frac{1}{M-1} \sum_{k=1}^{M-2} x[n-k]
$$

$$
  y_{M-1}[n-1] = \frac{1}{M-1} \sum_{k=1}^{M-1} x[n-k]
$$

$$
  \sum_{k=0}^{M-1}x[n-k] = x[n] + \sum_{k=1}^{M-1}x[n-k]
$$

$$
  M y_M[n] = x[n] + (M-1)y_{M-1}[n-1]
$$

$$
  y_M[n] = \frac{M-1}{M} y_{M-1}[n-1] + \frac{1}{M} x[n]
$$

$$
  y_M[n] = \lambda y_{M-1}[n-1] + (1-\lambda)x[n], \quad \lambda = \frac{M-1}{M}
$$

\subsection{The Leaky Integrator}

\begin{itemize}
  \item when $M$ is large, $y_{M-1}[n] \approx y_M[n]$ (and $\lambda \approx 1$)
  \item try the filter
  $$
    y[n] = \lambda y[n-1] + (1-\lambda)x[n]
  $$
  \item filter is now recursive, since it uses its previous output value
\end{itemize}

\subsection{What about the impulse response?}

$$
  y[n] = \lambda y[n-1] + (1-\lambda)\delta[n]
$$

\begin{itemize}
  \item $y[n] = 0$ for all $n<0$
  \item $y[0] = \lambda y[-1] + (1-\lambda)\delta[0] = (1-\lambda)$
  \item $y[1] = \lambda y[0] + (1-\lambda)\delta[1] = \lambda(1-\lambda)$
  \item $y[2] = \lambda y[1] + (1-\lambda)\delta[2] = \lambda^2(1-\lambda)$
  \item $y[3] = \lambda y[2] + (1-\lambda)\delta[3] = \lambda^3(1-\lambda)$
  \item $\ldots$
\end{itemize}

\textbf{Impulse response:}
$$
  h[n] = (1-\lambda)\lambda^n u[n]
$$

<<m5.2-impulse-response, echo=TRUE>>=
lambda <- 0.9
n <- -10:30
h <- c(rep(0, 10), (1-lambda)*lambda^(0:30))
dsplot(n, h, xlab="n", ylab="h[n]")
@

\subsection{Leaky Integrator: why the name}

Discrete-time integrator is a boundless accumulator:

$$
  y[n] = \sum_{k=-\infty}^{n} x[k]
$$

We can rewrite the integrator as

$$
  y[n] = y[n-1] + x[n]
$$

To prevent ``explosion'' pick $\lambda < 1$

$$
  y[n] = \lambda y[n-1] + (1+\lambda)x[n]
$$

\begin{itemize}
  \item $\lambda y[n-1]$: keep only a fraction $\lambda$ of the accumulated value so far and forget (``leak'') a fraction $1-\lambda$
  \item $(1+\lambda)x[n]$: add only a fraction $1-\lambda$ of the current value to the accumulator
\end{itemize}

\section{Filter stability}

\subsection{Filter types according to impulse response}
\begin{itemize}
  \item Finite Impulse Response (FIR)
  \item Infinite Impulse Response (IIR)
  \item causal
  \item noncausal
\end{itemize}

\subsection{FIR}
\begin{itemize}
  \item impulse response has finite support
  \item only a finite number of samples are involved in the computation of each output sample
\end{itemize}

\textbf{An example:} Moving Average filter

<<m5.3-fir, echo=TRUE>>=
M <- 8
n <- -30:30
x <- ifelse(n >= 0 & n < M, 1/M, 0)
dsplot(n, x)
@

\subsection{IIR}
\begin{itemize}
  \item impulse response has infinite support
  \item a potentially infinite number of samples are involved in the computation of each output sample
  \item surprisingly, in many cases the computation can still be performed in a finite amount of steps
\end{itemize}

\textbf{An example}: Leaky Integrator

<<m5.3-iir, echo=TRUE>>=
lambda <- 0.9
n <- -10:30
h <- c(rep(0, 10), (1-lambda)*lambda^(0:30))
dsplot(n, h, xlab="n", ylab="h[n]")
@

\subsection{Causal vs Noncausal}
\begin{itemize}
  \item causal:
    \begin{itemize}
      \item impulse response is zero for $n<0$
      \item only past samples (with respect to the present) are involved in the computation of each output sample
      \item causal filters can work ``on line'' since they only need the past
    \end{itemize}
  \item noncausal:
    \begin{itemize}
      \item impulse response is nonzero for some (or all) $n<0$
      \item can still be implemented in a offline fashion (when all input data is available on storage, e.g. in Image Processing)
    \end{itemize}
\end{itemize}

\subsection{Stability}
\begin{itemize}
  \item key concept: avoid ``explosions'' if the input is nice
  \item a nice signal is a bounded signal: $|x[n]| < M$ for all $n$
  \item Bounded-Input Bounded-Output (BIBO) stability: if the input is nice the output should be nice
\end{itemize}

\subsection{Fundamental Stability Theorem}
A filter is BIBO stable if and only if its impulse response is absolutely summable

\textbf{Proof $\Rightarrow$}

Hypotheses: $|x[n]| < M$ and $\sum_n |h[n]| = L < \infty$

Thesis: $|y[n]|$ bounded

Proof:

$$
|y[n]| = \Big| \sum_{k=-\infty}^{\infty} h[k]x[n-k] \Big| \leqslant
\sum_{k=-\infty}^{\infty} |h[k]x[n-k]| \leqslant
M \sum_{k=-\infty}^{\infty} |h[k]| \leqslant
ML
$$

\textbf{Proof $\Leftarrow$}

Hypotheses: $|x[n]| < M$ and $|y[n]| < P$

Thesis: $h[n]$ absolutely summable

Proof (by contradiction):
\begin{itemize}
  \item assume $\sum_n |h[n]|=\infty$
  \item build $x[n] = \begin{cases}
  +1 & \textrm{if} h[-n] \geqslant 0 \\
  -1 & \textrm{if} h[-n] < 0
  \end{cases}$
  \item clearly, $x[n]$ is bounded
  \item however
  $$
    y[0] = (x*h)[0] = 
    \sum_{k=-\infty}^{\infty} h[k]x[-k] = 
    \sum_{k=-\infty}^{\infty} |h[k]| = 
    \infty
  $$
\end{itemize}

\textbf{The good news}: FIR filters are always stable

\subsection{Checking the stability of IIRs}

Let's check the Leaky Integrator:

$$
  \sum_{n=-\infty}{\infty} |h[n]| = 
  |1-\lambda| \sum_{n=0}^{\infty} |\lambda|^n =
  \lim_{n\to\infty} |1-\lambda|\frac{1-|\lambda|^{n+1}}{1-|\lambda|} <
  \infty \; \textrm{for} \; |\lambda| < 1
$$

stability is guaranteed for $|\lambda| < 1$.

We will study indirect methods for filter stability later in this Module.

\section{Frequency response}

\subsection{A remarkable result}
$$
  e^{j\omega_0 n} \quad \longmapsto \quad \boxed{\mathcal{H}} \quad \longmapsto \quad ?
$$

$$
  y[n] = 
  e^{j\omega_0 n} * h[n] =
  h[n] * e^{j\omega_0 n} =
  \sum_{k=-\infty}^{\infty} h[k] e^{j\omega_0 (n-k)}=
  e^{j\omega_0 n} \sum_{k=-\infty}^{\infty} h[k] e^{-j\omega_0 k} =
  H(e^{j\omega_0})e^{j\omega_0 n}
$$

$$
  e^{j\omega_0 n} 
  \quad \longmapsto \quad 
  \boxed{\mathcal{H}} 
  \quad \longmapsto \quad 
  H(e^{j\omega_0})e^{j\omega_0 n}
$$

\begin{itemize}
  \item complex exponentials are \textit{eigensequences} of LTI systems, i.e., linear filters cannot change the frequency of sinusoids
  \item DTFT of impulse response determines the frequency characteristic of a filter
\end{itemize}

\subsection{Magnitude and phase}

if $H(e^{j\omega_0}) = A e^{j\theta}$, then $\mathcal{H} \{ e^{j\omega_0 n} \} = A e^{j (\omega_0 n + \theta)}$

Amplitude: amplification ($A>1$) or attenuation ($0 \leqslant A < 1$).

Phase shift: delay ($\theta < 0$) or advancement ($\theta > 0$).

\subsection{The convolution theorem}

In general:

$$
  DTFT \{ x[n] * h[n] \} = ?
$$

Intuition: the DTFT reconstruction formula tells us how to build $x[n]$ from a set of complex exponential ``basis'' functions. By linearity $x(e^{j\omega})e^{j\omega n} \longmapsto x(e^{j\omega})H(e^{j\omega})e^{j\omega n}$.

$$
\begin{array}{c}
  DTFT \{ x[n] * h[n] \} =
  \sum_{n=-\infty}^{\infty} (x*h)[n] e^{-j\omega n} =
  \sum_{n=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} x[k] h[n-k] e^{-j\omega n} = \\
  \sum_{n=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} x[k] h[n-k] e^{-j\omega (n-k)} e^{-j\omega k}=
  \sum_{k=-\infty}^{\infty} x[k] e^{-j\omega k} \sum_{n=-\infty}^{\infty} h[n-k] e^{-j\omega (n-k)} =
  H(e^{j\omega}) X(e^{j\omega})
\end{array}
$$

\subsection{Frequency response}
$$
  H(e^{j\omega}) = DTFT \{ h[n] \}
$$

Two effects:
\begin{itemize}
  \item \textbf{magnitude}: amplification ($|H(e^{j\omega})| > 1$) or attenuation ($|H(e^{j\omega})|<1$) of input frequencies
  \item \textbf{phase}: overall daly and shape changes
\end{itemize}

\subsection{Moving Average revisited}
$$
  h[n] = (u[n] - u[n-M]) / M
$$

$$
  |H(e^{j\omega})| = \frac{1}{M} \Big| \frac{\sin{(\frac{\omega}{2}M)}}{\sin{(\frac{\omega}{2})}} \Big|
$$

\subsection{What about the phase?}

Assume $|H(e^{j\omega})|=1$
\begin{itemize}
  \item zero phase: $\angle H(e^{j\omega})=0$
  \item linear phase: $\angle H(e^{j\omega})=d \omega$
  \item nonlinear phase
\end{itemize}

\subsection{Linear phase}

$$
  x[n] \quad \longmapsto \quad \boxed{\mathcal{D}} \quad \longmapsto \quad x[n-d]
$$

\begin{itemize}
  \item $y[n]=x[n-d]$
  \item $Y(e^{j\omega})=e^{-j\omega d}X(e^{j\omega})$
  \item $H(e^{j\omega})=e^{-j\omega d}$
  \item linear phase term
\end{itemize}

\section{Ideal filters}

\subsection{Filter types according to magnitude response}
\begin{itemize}
  \item Lowpass
  \item Highpass
  \item Bandpass
  \item Allpass
\end{itemize}

Moving Average and Leaky Integrator are lowpass filters.

\subsection{Filter types according to phase response}
\begin{itemize}
  \item Linear phase
  \item Nonlinear phase
\end{itemize}

\subsection{Ideal lowpass filter}

$$
H(e^{j\omega}) = \begin{cases}
1 & \textrm{for} |\omega| \leqslant \omega_c \\
0 & \textrm{otherwise}
\end{cases}
$$

\begin{itemize}
  \item perfectly flat passband
  \item infinite attenuation in stopband
  \item zero-phase (no delay)
\end{itemize}

\textbf{Impulse response}

$$
h[n] = IDTFT \{ H(e^{j\omega}) \} =
\frac{1}{2\pi} \int_{-\pi}^{\pi} H(e^{j\omega n} d \omega) =
\frac{1}{2\pi} \int_{-\omega_c}^{\omega_c} e^{j\omega n} d \omega =
\frac{1}{\pi n} \frac{e^{j\omega_c n} - e^{-j\omega_c n}}{2j} =
\frac{\sin \omega_c n}{\pi n}
$$

<<m5.5-iir, echo=TRUE>>=
wc <- 0.7
n <- seq(-25, 25, by=0.6)
h <- sin(wc*n)/(pi*n)
dsplot(n, h)
@

\textbf{The bad news:}

\begin{itemize}
  \item impulse response is infinite support, two sided: cannot compute the output in a finite amount of time; that's why it's called ``ideal''.
  \item impulse response decays slowly in time: we need a lot of samples for a good approximation.
\end{itemize}

\textbf{The sinc-rect pair:}

$$
\textrm{rect}(x) = \begin{cases}
1 & |x| \leqslant 1/2 \\
0 & |x| > 1/2
\end{cases}
$$

$$
\textrm{sinc}(x) = \begin{cases}
\frac{\sin \pi x}{\pi x} & x \neq 0 \\
1 & x = 0
\end{cases}
$$

(note that $\textrm{sinc}=0$ when $x$ is a nonzero integer)

\textbf{The ideal lowpass in canonical form}

$$
 \textrm{rect} \Big( \frac{\omega}{2\omega_c} \Big) 
 \leftrightarrow
 \frac{\omega_c}{\pi} \textrm{sinc} \Big( \frac{\omega_c}{\pi} n \Big)
 \quad\quad
 (DTFT)
$$

\textbf{Little-known fact}

\begin{itemize}
  \item the sinc is not absolutely summable
  \item the ideal lowpass is not BIBO stable!
  \item example for $\omega_c = \pi/3$: $h[n] = (1/3)\textrm{sinc}(n/3)$
  \item take $x[n] = \textrm{sign} \{ \textrm{sinc} (-n/3) \}$ and
  $$
    y[0] = (x*h)[0] = \frac{1}{3} \sum_{k=-\infty}^{\infty} |\textrm{sinc}(k/3)| = \infty
  $$
\end{itemize}

\subsection{Ideal highpass filter}

$$
H_{hp}(e^{h\omega}) = \begin{cases}
  1 & \textrm{for} \; \pi \geqslant |\omega| \geqslant \omega_c \\
  0 & \textrm{otherwise}
\end{cases}
$$

$$
  H_{hp}(e^{j\omega}) = 1 - H_{lp}(e^{j\omega})
$$

$$
  h_{hp}[n] = \delta[n] - \frac{\omega_c}{\pi} \textrm{sinc} \Big( \frac{\omega_c}{\pi} n \Big)
$$

\subsection{IDeal bandpass filter}

$$
  H_{bp}(e^{j\omega}) = \begin{cases}
    1 & \textrm{for} \; |\omega\pm \omega_0| \leqslant \omega_c \\
    0 & \textrm{otherwise}
  \end{cases}
$$

$$
  h_{bp} [n] = 2 \cos(\omega_0 n) \frac{\omega_c}{\pi} \textrm{sinc} \Big( \frac{\omega_c}{\pi} n \Big)
$$

\subsection{Demodulation revisited}

\begin{itemize}
  \item apply sinusoidal modulation to $x[n]: y[n] = x[n] \cos \omega_0 n$
  \item demodulate by multiplying by the carrier $x'[n] = y[n] \cos \omega_0 n$
  \item demodulated signal contains unwanted high-frequency components
\end{itemize}

\section{Filter design}

\subsection{How can we approximate an ideal lowpass?}

Idea \#1:
\begin{itemize}
  \item pick $\omega_c$
  \item compute ideal impulse response $h[n]$
  \item truncate $h[n]$ to a finite-support $\hat{h}[n]$
  \item $\hat{h}[n]$ defines an FIR filter
\end{itemize}

\textbf{Approximation by truncation}

FIR approximation of length $M=2N+1$ :

$$
\hat{h}[n] = \begin{cases}
  \frac{\omega_c}{\pi} \textrm{sinc} \Big( \frac{\omega_c}{\pi} n \Big) & |n| \leqslant N \\
  0 & \textrm{otherwise}
\end{cases}
$$

Why it could be a good idea:

$$
MSE = \frac{1}{2\pi} \int_{-\pi}^{\pi} |H(e^{j\omega}) - \hat{H}(e^{j\omega})|^2 d\omega =
||H(e^{j\omega}) - \hat{H}(e^{j\omega})||^2 =
||h[n] - \hat{h}[n]||^2 = 
\sum_{n=-\infty}^{\infty} |h[n] - \hat{h}[n]|^2
$$

MSE is minimized by symmetric impulse truncation around zero.

\textbf{What if we change the window?}

We want:

\begin{itemize}
  \item narrow mainlobe so that transition is sharp
  \item small sidelobe so Gibbs error is small
  \item short window so FIR is efficient
\end{itemize}

very conflicting requirements!

\subsection{Frequency sampling}

Idea \#2:
\begin{itemize}
  \item draw desired frequency response $H(e^{j\omega})$
  \item take $M$ values at $\omega_k = (2\pi / M)k$
  \item compute IDFT of values
  \item use result as an $M$-tap impulse response $\hat{h}[n]$
\end{itemize}

\newpage

\section{Realizable filters}

\subsection{The most general LTI systems}

\begin{itemize}
	\item ideal filters cannot be implemented
	\item what is the most general, realizable LTI transformation?
	\begin{itemize}
		\item linearity: only sums and multiplications
		\item time-invariance: only multiplications by constants
		\item realizability: only finite number of past and future samples
	\end{itemize}
\end{itemize}

\subsection{Constant-Coefficient Difference Equation}

$$
\sum_{k=0}^{N-1} a_k y[n-k] = 
\sum_{k=0}^{M-1} b_k x[n-k]
$$

\begin{itemize}
	\item uses $M$ input and $N$ output values
	\item how do we compute the frequency response?
	\item we need a new tool!
\end{itemize}

\subsection{The z-transform}

$$
  X(z) = \sum_{-\infty}^{\infty} x[n] z^{-n}, \quad z \in \mathbb{C}
$$

\begin{itemize}
  \item think of it as a formal operator...
  \item ... or as the extension of the DTFT to the whole complex plane:
  $$
    X(z)|_{z=e^{j\omega}} = DTFT \{ x[n] \}
  $$
\end{itemize}

Linearity:

$$
  \mathcal{Z} \{ \alpha x[n] + \beta y[n] \} =
  \alpha X(z) + \beta Y(z)
$$

Time shift:

$$
  \mathcal{Z} \{ x[n - N] \} =
  z^{-N} X(z)
$$

\subsection{Applying the z-transform to CCDE's}

$$
\sum_{k=0}^{N-1} a_k y[n-k] = 
\sum_{k=0}^{M-1} b_k x[n-k]
$$

$$
Y(z) \sum_{k=0}^{N-1} a_k z^{-k} = 
X(z) \sum_{k=0}^{M-1} b_k z^{-k}
$$

$$
Y(z) = H(z) X(z)
$$

Rational Transfer Function:

$$
H(z) = \dfrac{\sum_{k=0}^{M-1} b_k z^{-k}}{\sum_{k=0}^{N-1} a_k z^{-k}}
$$

by setting $z=e^{j\omega}$ we have the frequency response! (and now the notation $X(e^{j\omega})$ should make more sense).

\subsection{Leaky Integrator revisited}

$$
y[n] = (1 - \lambda) x[n] + \lambda y[n-1]
$$

$$
Y(z) = (1 - \lambda) X(z) + \lambda z^{-1} Y(z)
$$

$$
H(z) = \frac{1-\lambda}{1 - \lambda z^{-1}}
$$

$$
H(e^{j\omega}) = \frac{1-\lambda}{1 - \lambda e^{-j\omega}}
$$

\subsection{Region of convergence (ROC)}

the z-tranform is a power series, so convergence is always absolute

$$
|X(z)| < \infty \Longleftrightarrow \sum_{n=\infty}^{\infty} |x[n]z^{-n}| < \infty
$$

\begin{itemize}
  \item ROC is whole complex plane for finite-support sequences
  \item ROC has circular symmetry (depends only on |z|)
  \item ROC of causal sequences extends from a circle of infinity
\end{itemize}

ROC for causal systems:

\begin{itemize}
  \item $z_n$'s zeros of the transfer function
  \item $p_n$'s poles of the transfer function
  \item only trouble spots for ROC are the poles
\end{itemize}

\section{Implementation of digital filters}

\subsection{Leaky integrator}

<<m5.8-leaky, echo=TRUE>>=
lambda <- 0.95
y <- 0
leaky <- function(x) {
  y <<- lambda * y + (1 - lambda) * x
  y
}
for (i in 0:19) {
  cat(sprintf("%.4f\n", leaky(ifelse(i == 0, 1.0, 0.0))))
}
@

Key points:

\begin{itemize}
  \item we need a ``memory cell'' to store previous output
  \item we need to initialize the storage before first use
  \item we need 2 multiplications and one addition per output sample
\end{itemize}

Better implementation:

<<m5.8-leaky2, echo=TRUE>>=
leaky <- function(lambda, x) {
	y <- 0
	res <- c()
	for (v in x) {
		y <- lambda * y + (1 - lambda) * v
		res <- c(res, y)
	}
	res
}
y <- leaky(0.95, c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0))
y
dsplot(-4:6, y)
@

\subsection{Moving average}

$$
  y[n] = \frac{1}{M} \sum_{k=0}^{M-1} x[n-k]
$$

Key point:

\begin{itemize}
  \item we now need $M$ memory cells to store previous input values
  \item we need to initialize the storage before first use
  \item we need $1$ division and $M$ additions per output sample
\end{itemize}

Implementation:

<<m5.8-moving-average, echo=TRUE>>=
movingAverage <- function(M, x) {
  z <- rep(0, M)
  ix <- 0
  res <- c()
  for (v in x) {
    z[ix] <- v
    ix <- (ix + 1) %% M
    avg <- mean(z)
    res <- c(res, avg / M)
  }
  res
}
y <- movingAverage(10, rep(1, 20))
y
dsplot(0:19, y)
@

\section{Filter design --- Part 2: Intuitive Filters}

\subsection{Simple lowpass}

\begin{itemize}
  \item let only low frequencies pass
  \item used to remove high frequency components (e.g. noise)
  \item useful in audio, communication, control systems
  \item we know a simple answer: leaky integrator
\end{itemize}

\subsection{Leaky Integrator}

$$
H(z) = \frac{1-\lambda}{1 - \lambda z^{-1}}
$$

$$
y[n] = (1-\lambda) x[n] + \lambda y[n-1]
$$

\subsection{Resonator}

\begin{itemize}
  \item a resonator is a narrow bandpass filter
  \item used to detect the presence of a sinusoid of a given frequency
  \item useful in communication systems and telephony (DTMF)
  \item idea: shift the passband of the Leaky Integrator!
\end{itemize}

$$
  H(z) = \frac{G_0}{(1-pz^{-1})(1-p^*z^{-1})}, \; p = \lambda e^{j\omega_0}
$$

$$
  y[n] = G_0 x[n] - a_1 y[n-1] - a_2 y[n-2]
$$

Canonical form:

$$
H(z) = \frac{G_0}{1-2\lambda \cos \omega_0 z^{-1} + |\lambda|^2 z^{-2}}
$$
$$
a_1 = 2\lambda \cos \omega_0, \quad a_2 = -|\lambda|^2
$$

\subsection{DC removal}

\begin{itemize}
  \item a DC-balanced signal has zero sum:
  $$
    \lim_{N \to \infty} \sum_{n=-N}^{N} x[n] = 0
  $$
  i.e. there is no Direct Current component
  \item its DTFT value at zero is zero
  \item we want to removal the DC bias from a non zero-centered signal
  \item we want to kill the frequency component at $\omega=0$
\end{itemize}

Improved version:

$$
H(z) = \frac{1-z^{-1}}{1-\lambda z^{-1}}
$$

$$
y[n] = \lambda y[n-1] + x[n] - x[n-1]
$$

\subsection{Hum removal}

\begin{itemize}
  \item similar to DC removal but we want to remove a specific nonzero frequency
  \item very useful for musicians: amplifiers for electric guitar pick up the hum from the electric mains (50Hz in Eurpoe and 60Hz in North America)
  \item we need to tune the hum removal according to country
\end{itemize}

$$
H(z) = \frac{(1-e^{j\omega_0} z^{-1})(1-e^{-j\omega_0} z^{-1})}{(1-\lambda e^{j\omega_0} z^{-1})(1- \lambda e^{-j\omega_0} z^{-1})}
$$

\section{Filter design --- Part 3: Design from specs}

\subsection{The filter design problem}

You are given a set of requirements:

\begin{itemize}
  \item frequency response: passband(s) and stopband(s)
  \item phase: overall delay, linearity
  \item some limit on computational resources and/or numerical precision
\end{itemize}

You must determine $N$, $M$, $a_k$'s and $b_k$'s in

$$
H(z) = \frac{b_0 + b_1 z^{-1} + \ldots + b_{M-1} z^{-(M-1)}}{a_0 + a_1 z^{-1} + \ldots + a_{M-1} z^{-(M-1)}}
$$

in order to best fulfill the requirements.

\subsection{Practical limitations}

\begin{itemize}
  \item passband/stopband transitions cannot be infinitely sharp => use transition bands
  \item magnitude response cannot be constant over an interval
  \item in general:
    \begin{itemize}
    	\item small transition bands => hight filter order
    	\item small error tolerance => higher filter order
    	\item higher filter order => more expensive, larger delay
    \end{itemize}
\end{itemize}

\section{Real-time processing}

Everything works in sync with a system clock of period $T_s$:
\begin{itemize}
  \item ``record'' a value $x_i[n]$
  \item process the value in a causal filter
  \item ``play'' the output $x_0[n]$
\end{itemize}
everything needs to happen in a t most $T_s$ seconds!


\subsection{Buffering}

\begin{itemize}
  \item interrupt for each sample would be too much overhead
  \item soundcard consumes sample in buffers
  \item soundcard notifies when buffer used up
  \item CPU can fill a buffer in less time than soundcard can empty it
\end{itemize}

buffering introduces delay!


\section{Dereverbation and echo cancelation}

\subsection{Sound Propagation}

\begin{itemize}
  \item Free space propagation, sound pressure $\approx \frac{1}{\textrm{distance}}$
  \item Every reflection attenuates the sound by $\alpha \in [0,1]$
  \item Room is a linear filter, it acts by convolution
  \item $y_{\textrm{listener}}[n] = x_{\textrm{source}}[n] * h[n; positions]$
  \item We will supress the dependence on source and listener positions for simplicity
\end{itemize}

\newpage
\chapter{Interpolation and Sampling}

\section{The continuous-time paradigm}

\subsection{Digital processing of signals from/to the analog world}

\begin{itemize}
  \item input in continuous-time: $x(t)$
  \item ouptut is continuous-time: $y(t)$
  \item processing is on sequences: $x[n], \, y[n]$
\end{itemize}

Examples: MP3, digital photography.

\subsection{Two views of the world}

Digital worldview:

\begin{itemize}
  \item arithmetic
  \item combinatorics
  \item computer science
  \item DSP
\end{itemize}

Analog worldview:

\begin{itemize}
  \item calculus
  \item distributions
  \item system theory
  \item electronics
\end{itemize}

Digital worldview:

\begin{itemize}
  \item countable integer index $n$
  \item sequences $x[n] \in l_2 (\mathbb{Z})$
  \item frequency $\omega \in [-\pi, \pi]$
  \item DTFT: $l_2(\mathbb{Z} \mapsto L_2 ([-\pi; \pi])$
\end{itemize}

Analog worldview:

\begin{itemize}
  \item real-valued time $t$ (sec)
  \item functions $x(t) \in L_2(\mathbb{R})$
  \item frequency $\Omega \in \mathbb{R}$ (rad/sec)
  \item FT: $L_2(\mathbb{R}) \mapsto L_2(\mathbb{R})$
\end{itemize}

\subsection{About continuous time}

\begin{itemize}
  \item time: real variable $t$
  \item signal $x(t)$: complex functions of a real variable
  \item finite energy: $x(t) \in L_2(\mathbb{R})$
  \item inner product in $L_2(\mathbb{R})$
    $$
      \langle x(t), y(t) \rangle =
      \int_{-\infty}^{\infty} x^*(t)y(t)dt
    $$
  \item energy: $||x(t)||^2 = \langle x(t), x(t) \rangle$
\end{itemize}

Analog LTI filters:

$$
y(t) = 
(x*h)(t) =
\langle h^*(t-\tau), x(\tau) \rangle =
\int_{-\infty}^{\infty} x(\tau)h(t-\tau)d\tau
$$

\subsection{Fourier analysis}
\begin{itemize}
  \item in discrete time max angular frequency is $\pm \pi$
  \item in continuous no max frequency: $\Omega \in \mathbb{R}$
  \item concept is the same:
    $$
      X(j\Omega) = \int_{-\infty}^{\infty} x(t) e^{-j\Omega t} dt
    $$
    $$
      x(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} X(j\Omega) e^{j\Omega t} d \Omega
    $$
\end{itemize}

\subsection{Real-world frequency}

\begin{itemize}
  \item $\Omega$ expressed in rad?s
  \item $F=\frac{\Omega}{2\pi}$, expressed in Hertz (1/s)
  \item period $T=\frac{1}{F}=\frac{2\pi}{\Omega}$
\end{itemize}

Example:

$$
  x(t)=e^{-\frac{t^2}{2\sigma^2}}
$$

<<m6.1-gauss, echo=TRUE>>=
sigma <- 3
t <- seq(-50, 50, by=0.1)
x <- exp(-(t^2)/(2*sigma^2))
csplot(t, x)
@

$$
  X(j\omega)=\sigma \sqrt{2\pi} e^{-\frac{\sigma^2}{2}\Omega^2}
$$

<<m6.1-gauss-ft, echo=TRUE>>=
sigma <- 3
omega <- seq(-50, 50, by=0.1)
X <- sigma * sqrt(2*pi) * exp(-(sigma^2)*(omega^2)/2)
csplot(omega, X)
@

\subsection{Convolution theorem}

$$
Y(j\Omega) = X(j\Omega) H(j\Omega)
$$

\subsection{Bandlimited functions}

$\Omega_N$-bandlimitedness:

$$
  X(j\Omega) = 0 \quad \textrm{for} \; |\Omega| > \Omega_N
$$

\subsection{Prototypical bandlimited function}

\begin{itemize}
  \item normailization: $G=\frac{\pi}{\Omega_N}$
  \item total bandwidth: $\Omega_B = 2\Omega_N$
  \item define $T_s = \frac{2\pi}{\Omega_B} = \frac{\pi}{\Omega_N}$
\end{itemize}

\section{Interpolation}

Main task:

$$
  x[n] \mapsto x(t)
$$

``fill the gaps'' between samples

\subsection{Interpolation requirements}

\begin{itemize}
  \item decide on $T_s$
  \item make sure $x(nT_s) = x[n]$
  \item make sure $x(t)$ is smooth
\end{itemize}

\subsection{Why smoothness}

\begin{itemize}
  \item jumps (1st order discontinuities) would require the signal to move ``fatser than light''...
  \item 2nd order discontinuities would require infinite acceleration
  \item the interpolation should be infinitely differentiable
  \item ``natual'' solution: polynomial interpolation
\end{itemize}

\subsection{Polynomial interpolation}

\begin{itemize}
  \item $N$ points $\mapsto$ polynomial of degree $(N-1)$
  \item $p(t) = a_0 + a_1 t + a_2 t^2 + \ldtos + a_{N-1} t^{N-1}$
  \item ``native'' approach:
  $$
    \left\{
      \begin{array}{rcl}
        p(0) & = & x[0] \\
        p(T_s) & = & x[1] \\
        p(2T_s) & = & x[2] \\
        & \ldots & \\
        p((N-1)T_s) & = & x[N-1]
      \end{array}
    \right.
  $$
\end{itemize}

Without loss of generality:

\begin{itemize}
  \item consider a symmetric interval $l_N = [-N,\ldots,N]$
  \item set $T_s=1$:
  $$
  \left\{
    \begin{array}{rcl}
      p(-N) & = & x[-N] \\
      p(-N+1) & = & x[-N+1] \\
      & \ldots & \\
      p(0) & = & x[0] \\
      & \ldots & \\
      p(N) & = & x[N]
    \end{array}
  \right.
  $$
\end{itemize}

\subsection{Lagrange interpolation}

\begin{itemize}
  \item $P_N$: space of degree-$2N$ polynomials over $l_N$
  \item a basis for $P_N$ is the family of $2N+1$ Lagrange polynomials
  $$
    L_n^{(N)} (t) = \prod_{k=-N, k\neq n}^N \frac{t-k}{n-k}, \quad
    n = -N, \ldots, N
  $$
\end{itemize}

The formula:

$$
  p(t) = \sum_{n=-N}^{N} x[n] L_n^{(N)} (t)
$$

\subsection{Local interpolation schemes}

$$
x(t) = \sum_{n=-N}^{N} x[n] i_c(t-n)
$$

Interpolator's requirements:

\begin{itemize}
  \item $i_c(0) = 1$
  \item $i_c(t) = 0$ for $t$ a nonzero integer
\end{itemize}

Key property: same interpolating function independently of $N$.

Drawback: lack of smoothness.

\subsection{Sinc interpolation formula}

$$
  x(t) = \sum_{n=-\infty}^{\infty} x[n] sinc \Big( \frac{t-nT_s}{T_s} \Big)
$$

\section{The space of bandlimited signals}

\subsection{Spectral representation}

$$
  X(j\Omega) = \begin{cases}
    (\pi/\Omega_N)X(e^{j\pi(\Omega-\Omega_N)} & \textrm{for} \; |\Omega| \leqslant |\Omega_N| \\
    0 & \textrm{otherwise}
  \end{cases}
$$

\subsection{The space of $\pi$-Bl functions}

Inner product:

$$
\langle x(t), y(t) \rangle = \int_{-\infty}^{\infty} x^*(t) y(t) dt
$$

Convolution:
$$
  (x*y)(t) = \langle x^*(\tau), y(t-\tau) \rangle
$$

A basis for the $\pi$-BL space:

$$
\phi^{(n)}(t) = \textrm{sinc} (t-n), \quad n \in \mathbb{Z}
$$

$$
\textrm{FT} \{ \textrm{sinc}(t) \} = \textrm{rect} \Big( \frac{\Omega}{2\pi} \Big)
$$

$$
(\textrm{sinc}*\textrm{sinc})(m-n) = \begin{cases}
  1 & \textrm{for} m=n \\
  0 & \textrm{otherwise}
\end{cases}
$$

$$
x(t) = \frac{1}{T_s} \sum_{n=-\infty}^{\infty} x[n] \textrm{sinc}
\Big( \frac{t-nT_s}{T_s} \Big)
$$

\section{Sampling and aliasing: Introduction}

\subsection{Sinc sampling}

$$
x[n] = (\textrm{sinc}_{T_s} *x)(nT_s)
$$

\subsection{Raw sampling}

$$
x[n] = x(nT_s)
$$

\subsection{The continuous-time complex exponential}

$$
x(t) = e^{j\Omega_0 t}
$$


\section{Sampling and aliasing}

\subsection{Spectrum of raw-sampled signals}

Start with the inverse Fourier Transform:

$$
x[n] = x_c(nT_s) = 
\frac{1}{2\pi} \int_{-\infty}^{\infty} X_c(j\Omega) e^{j\Omega n T_s} d\Omega
$$

frequencies $2\Omega_N$ apart will be aliased, so split the integration interval:

$$
x[n] = \frac{1}{2\pi} \sum_{k=\infty}^{\infty} \int_{(2-k-1)\Omega_N}^{(2k+1)\Omega_N}
X_c(j\Omega) e^{j\Omega n T_s} d\Omega
$$

\section{Descrete-time processing of continuous-time signals}

\chapter{Stochastic Signal Processing and Quantization}

\section{Stochastic signal processing}

\subsection{Deterministic vs. stochastic}
\begin{itemize}
  \item deterministic signals are known in advance: $x[n] = \sin(0.2 n)$
  \item interesting signals are not known in advance: $s[n] = $ what I'm going to say next
  \item we usually know something, though: $s[n]$ is a speech signal
  \item stochastic signals can be described probabilistically
  \item can we do signal processing with random signals? Yes!
  \item will not develop stochastic signal processing rigorously but give enough intuition to deal with things such as ``noise''
\end{itemize}

\subsection{A simple discrete-time random signal generator}

For each new sample, toss a fair coin:
$$
x[n] = \begin{cases}
  +1 & \textrm{if the outcome of the n-th toss is head} \\
  -1 & \textrm{if the outcome of the n-th toss is tail}
\end{cases}
$$

\begin{itemize}
  \item each sample is independent from all others
  \item each sample value has a $50\%$ probability
  \item every time we turn on the generator we obtain a different realization of the signal
  \item we know the ``mechanism'' behind each instance
  \item but how can we analyze a random signal?
\end{itemize}

<<m7.1-random, echo=TRUE>>=
t <- 0:31
for (seed in 1:3) {
  set.seed(seed)
  x <- sample(c(-1, 1),32,replace=T)
  print(dsplot(t, x))
}
@

\subsection{Spectral properties?}

\begin{itemize}
  \item let's try with the DFT of a finite set of random samples
  \item every time it's different; maybe with more data?
  \item no clear pattern... we need a new strategy
\end{itemize}

<<m7.1-random-dft, echo=TRUE>>=
for (seed in 1:3) {
  set.seed(seed)
  x <- sample(c(-1, 1),32,replace=T)
  dftplot(x)
}
set.seed(4)
x <- sample(c(-1, 1),128,replace=T)
dftplot(x)
@

\subsection{Averaging}

\begin{itemize}
  \item when faced with random data an intuitive response is to take ``averages''
  \item in probability theory the average is across realizations and it's called expectation
  \item for the coin-toss signal:
  $$
    E[x[n]] = -1 \cdot P[\textrm{n-th toss is tail}] + 1 \cdot P[\textrm{n-th toss is head}] = 0
  $$
  \item so the average value for each sample is zero
\end{itemize}

\textbf{Averaging the DFT}

\begin{itemize}
  \item ... as a consequence, averaging the DFT will not work
  \item $E[X[k]]=0$
  \item however the signal ``moves'', so its energy or power must be nonzero
\end{itemize}

\textbf{Energy and power}

\begin{itemize}
  \item the coin-toss signal has infinite energy (see Module 2.1):
  $$
    E_x = 
    \lim_{N \to \infty} \sum_{n=-N}^{N} |x[n]|^2 = 
    \lim_{N \to \infty} (2N+1) =
    \infty
  $$
  \item however it has finite power over any interval:
  $$
    P_x =
    \lim_{N \to \infty} \frac{1}{2N+1} \sum_{n=-N}^{N} |x[n]|^2 =
    1
  $$
\end{itemize}

\textbf{Averaging}

let's try to average the DFT's square magnitude, normalized:
\begin{itemize}
  \item pick an interval length $N$
  \item pick a number of iterations $M$
  \item run the signal generator $M$ times and obtain $M$ $N$-point realizations
  \item compute the DFT of each realization
  \item average their square magnitude divided by $N$
\end{itemize}

\subsection{Power spectral density}

$$
  P[k] = E[|X_n[k]|^2/N|
$$

\begin{itemize}
  \item it looks very much as if $P[k] = 1$
  \item if $|X_N[k]|^2$ tends to the energy distribution in frequency...
  \item ... $|X_N[k]|^2/N$ tends to the power distribution (aka density) in frequency
  \item the frequency-domain representation for stochastic processes is the power spectral density
\end{itemize}

\textbf{Power spectral density: intuition}

\begin{itemize}
  \item $P[k]=1$ means that the power is equally distributed over all frequencies
  \item i.e., we cannot predict if the signal moves ``slowly'' or ``super-fast''
  \item this is because each sample is independent of each other: we could have a realization of all ones or a realization in which the sign changes every other samples or anything in between
\end{itemize}

\subsection{Filtering a random process}

\begin{itemize}
  \item let's filter the random process with a 2-point Moving Average filter
  \item $y[n] = (x[n] + x[n-1])/2$
  \item what is the power spectral density?
  \item it looks like $P_y[k] = P_x[k] |H[k]|^2$, where $H[k] = DFT\{ h[n] \}$
  \item can we generalize these results beyond a finite set of samples?
\end{itemize}

\subsection{Stochastic signal processing}

\begin{itemize}
 \item a stochastic process is characterized by its power spectral density (PSD)
 \item is can be shown (see the textbook) that the PSD is
 $$
   P_x(e^{j\omega}) = DTFT \{ r_x[n] \}
 $$
 where $r_x[n] = E[x[k] x[n+k]]$ is the autocorrelation of the process.
 \item for a filtered stochastic process $y[n] = \mathcal{H} \{ x[n]\}$, it is:
 $$
   P_y (e^{j\omega}) = |H(e^{j\omega})|^2 P_x(e^{j\omega})
 $$
\end{itemize}

\textbf{Key points:}

\begin{itemize}
  \item filters designed for deterministic signals still work (in magnitude) in the stochastic case
  \item we lose the concept of phase since we don't know the shape of a realization in advance
\end{itemize}

\subsection{Noise}
\begin{itemize}
  \item noise is everywhere:
    \begin{itemize}
      \item thermal noise
      \item sum of extraneous interferences
      \item quantization and numerical errors
      \item ...
    \end{itemize}
  \item we can model noise as a stochastic signal
  \item the most important noise is white noise
\end{itemize}

\textbf{White noise}

\begin{itemize}
  \item ``white'' indicates uncorrelated samples
  \item $r_w[n] = \sigma^2 \delta[n]$
  \item $P_w(e^{j\omega}) = \sigma^2$
  \item the PSD is independent of the probability distribution of the single samples (depends only on the variance)
  \item distribution is important to estimate bounds for the signal
  \item very often a Gaussian distribution models the experimental data the best
  \item AWGN: additive white Gaussian noise
\end{itemize}

\section{Quantization}

\begin{itemize}
  \item digital devices can only deal with integers ($b$ bits per sample)
  \item we need to map the range of a signal onto a finite set of values
  \item irreversible loss of information $\to$ quantization noise
\end{itemize}

Several factors at play:
\begin{itemize}
  \item storage budget (bits per sample)
  \item storage scheme (fixed point, floating point)
  \item properties of the input: range and probability distribution
\end{itemize}

\subsection{Scalar quantization}

The simplest quantizer:
\begin{itemize}
  \item each sample is encoded individually (hence scalar)
  \item each sample is quantized independently (memoryless quantization)
  \item each sample is encoded using $R$ bits
\end{itemize}

Assume input signal bounded: $A \leqslant x[n] \leqslant B$ for all $n$:
\begin{itemize}
  \item each sample quantized over $2^R$ possible values => $2^R$ intervals.
  \item each interval associated to a quantization value
\end{itemize}

\subsection{Quantization Error}

$$
  e[n] = \mathcal{Q} \{ x[n] \} - x[n] = \hat x[n] - x[n]
$$

\begin{itemize}
  \item model $x[n]$ as a stochastic process
  \item model error as a white noise sequence: error samples are uncorrelated and all erorr samples have the same distribution
  \item we need statistics of the input to study the error
\end{itemize}

\subsection{Uniform quantization}

\begin{itemize}
  \item simple but very general case
  \item range is split into $2^R$ equal intervals of width $\Delta = (B-A)2^{-R}$
\end{itemize}

Mean Square Error is the variance of the error signal:

$$
  \sigma_e^2 = 
  E[|\mathcal{Q} \{ x[n] \} - x[n]|^2] =
  \int_A^B f_x(\tau) (\mathcal{Q} \{ \tau \} - \tau)^2 d\tau =
  \sum_{k=0}^{2^R-1} \int_{l_k} f_x(\tau) (\hat x_k - \tau)^2 d\tau
$$
error depends on the probability distribution of the input

\subsection{Uniform quantization of uniform input}

Uniform-input hypothesis:

$$
f_x(\tau) = \frac{1}{B-A}
$$

$$
\sigma_e^2 = \sum_{k=0}^{2^R-1} \int_{l_k} \frac{(\hat x_k - \tau)^2}{B-A} d\tau
$$

Let's find the optimal quantization point by minimizing the error:

$$
\frac{d\sigma_e^2}{d\hat{x}_m} = \frac{(\hat{x}_m - \tau)^2}{B-A} \Big|_{A+m\Delta}^{A+m\Delta+\Delta}
$$

Minimizing the error:

$$
\frac{d\sigma_e^2}{d\hat{x}_m} = 0 \quad \textrm{for} \quad \hat{x}_m = A + m\Delta + \Delta/2
$$

optimal quantization point is the interval's midpoint, for all intervals.

Quantizer's mean square error:

$$
\sigma_e^2 = \frac{\Delta}{12}
$$

\subsection{Error analysis}
\begin{itemize}
  \item Error energy: $\sigma_e^2 = \Delta^2/12, \quad \Delta=(B-A)/2^R$
  \item Signal energy: $\sigma_x^2 = (B-A)^2/12$
  \item Signal to noise ration: $\textrm{SNR} = 2^{2R}$
  \item in dB: $\textrm{SNR}_{dB} = 10 \log_{10} 2^{2R} \approx 6R\; dB$
\end{itemize}

\subsection{The ``6dB/bit'' rule of thumb}

\begin{itemize}
  \item a compact disk has 16 bits/sample: max SNR = 96dB
  \item a DVD has 24 bits/sample: max SNR = 144dB
\end{itemize}

\subsection{Other quantization errors}

If input is not uniform:

\begin{itemize}
  \item use uniform quantizer and accept increased error. For instance, if input is Gaussian:
  $$
    \sigma_e^2 = \frac{\sqrt{3} \pi}{2} \sigma^2 \Delta^2
  $$
  \item design optimal quantizer for input distribution, in known (Lloyd-Max algorithm)
  \item use ``companders''
\end{itemize}

\section{A/D and D/A conversion}

\chapter{Image Processing}

\section{Image Processing}

\subsection{Digital images}

\begin{itemize}
  \item two-dimensional signal $x[n_1, n_2], n_1, n_2 \in \mathbb{Z}$
  \item indices locate a point on a grid $\mapsto$ pixel
  \item grid is usually regularly spaced
  \item values $x[n_1, n_2]$ refer to the pixel's appearance
\end{itemize}

\subsection{Digital images: grayscale vs color}

\begin{itemize}
  \item grayscale images: scalar pixel values
  \item color images: multidimensional pixel values in a color space (RGB, HSV, YUV, etc)
  \item we can consider the single components separately
\end{itemize}

\subsection{Image processing}

From one to two dimensions...

\begin{itemize}
  \item something still works
  \item something breaks down
  \item something is new
\end{itemize}

What works:

\begin{itemize}
  \item linearity, convolution
  \item Fourier transform
  \item interpolation, sampling
\end{itemize}

What breaks down:

\begin{itemize}
  \item Fourier analysis less relevant
  \item filter design hard, IIRs rare
  \item linear operators only mildly useful
\end{itemize}

What's new:

\begin{itemize}
  \item new manipulations: affine transforms
  \item images are finite-support signals
  \item images are (most often) available in their entirety $\mapsto$ causality meaning
  \item images are very specialized signals, designed for a very specific processing system i.e. the human brain! Lots os semantics that is extermely hard to deal with
\end{itemize}

\subsection{Digital signal processing: the basics}

A two-dimensional descrete-space signal:  $x[n_1, n_2], \quad n_1, n_2 \in \mathbb{Z}$

\begin{itemize}
  \item just show coordinates of nonzero samples
  \item aplitude may be written along explicitly
  \item example:
  $$
    \delta[n_1, n_2] = \begin{cases}
      1 & \textrm{if}\; n_1=n_2=0 \\
      0 & \textrm{otherwise}
    \end{cases}
  $$
  \item medium has a certain dynamic range (paper, screen)
  \item image values are quantized (usually to 8 bits, or 256 levels)
  \item the eye does interpolation in space provided the pixel density in high enough
\end{itemize}

\subsection{Why 2D?}

\begin{itemize}
  \item images could be unrolled (printers, fax)
  \item but what amount spatial correlation?
\end{itemize}

\subsection{Separable signals}

$$
  x[n_1, n_2] = x_1[n_1]x_2[n_2]
$$

\subsection{Nonseparable signals}

Example:

$$
x[n_1, n_2] = \begin{cases}
  1 & \textrm{if}\; |n_1|+|n_2| < N \\
  0 & \textrm{otherwise}
\end{cases}
$$

\subsection{2D convolution for separable signals}

if $h[n_1, n_2]$ is $M_1 \times M_2$ finite-support signal:
\begin{itemize}
  \item non-separable convolution: $M_1M_2$ operations per output sample
  \item separable convolution: $M_1+M_2$ operations per output sample!
\end{itemize}

\section{Image manipulations}

\subsection{Affine transforms}

Mapping $\mathbb{R}^2 \mapsto \mathbb{R}^2$ that reshapes the coordinate system:

$$
\begin{bmatrix}
  t_1' \\ t_2'
\end{bmatrix} =
\begin{bmatrix}
  a_{11} & a_{12} \\
  a_{2_1} & a_{22}
\end{bmatrix}
\begin{bmatrix}
  t_1 \\ t_2
\end{bmatrix} -
\begin{bmatrix}
  d_1 \\ d_2
\end{bmatrix}
$$

$$
\begin{bmatrix}
  t_1' \\ t_2'
\end{bmatrix} =
A
\begin{bmatrix}
t_1 \\ t_2
\end{bmatrix} -
d
$$

\subsection{Translation}

$$
A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I, \quad d = \begin{bmatrix} d_1 \\ d_2 \end{bmatrix}
$$

\subsection{Scaling}

$$
A = \begin{bmatrix} a_1 & 0 \\ 0 & a_2 \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

\subsection{Rotation}

$$
A = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

\subsection{Flips}

Horizontal:

$$
A = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

Vertical:

$$
A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

\subsection{Shear}

Horizontal:

$$
A = \begin{bmatrix} 1 & s \\ 0 & 1 \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

Vertical:

$$
A = \begin{bmatrix} 1 & 0 \\ s & 1 \end{bmatrix} = I, \quad d = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

\subsection{Interpolation}

\begin{itemize}
  \item apply the inverse transform:
  $$
    \begin{bmatrix} t_1 \\ t_2 \end{bmatrix} =
    A^{-1}
    \begin{bmatrix} m_1+d_1 \\ m_2+d_2 \end{bmatrix}
  $$
  \item interpolate from the original grid point to the ``mid-point''
  $$
    (t_1, t_2) = (\eta_1 + \tau_1, \eta_2 + \tau_2), \quad \eta_{1,2} \in \mathbb{Z}, \quad 0 \leqslant \tau_{1,2} < 1
  $$
\end{itemize}

\subsection{Bilinear interpolation}

$$
  \begin{array}{rcl}
    y[m_1, m_2] &=& (1-\tau_1)(1-\tau_2) x[\eta_1, \eta_2] + \\
    ~&~&\tau_1(1-\tau_2) x[\eta_1+1, \eta_2]+ \\
    ~&~&(1-\tau_1) \tau_2 x[\eta_1, \eta_2+1] + \\
    ~&~&\tau_1 \tau_2 x[\eta_1 + 1, \eta_2 + 1]
  \end{array}
$$

\section{Frequency analisys}

\section{Image filtering}

\subsection{Analogies with 1D filters}
\begin{itemize}
  \item linearity
  \item space invariance
  \item impulse response
  \item frequency response
  \item stability
  \item 2D CCDE
\end{itemize}

\subsection{Filters}
\begin{itemize}
  \item Moving Average
  \item Gaussian Blur
  \item Sobel filter
  \item Laplician operator
\end{itemize}

\section{Image compression}

\subsection{Key ingredients}

\begin{itemize}
  \item compressing at block level
  \item using a suitable transform (i.e., a change of basis)
  \item smart quantization
  \item entropy coding
\end{itemize}

\section{The JPEG compression algorithm}

\subsection{Key ingredients}

\begin{itemize}
	\item split image into 8x8 non-overlapping blocks
	\item compute the DCT of each block
	\item quantize DCT coefficients according to psycovisually-tuned tables
	\item run-length encoding and Huffman coding
\end{itemize}

\appendix
\renewcommand{\chaptername}{Appendix}
\makeatletter
\renewcommand{\@chapapp}{Appendix}
\makeatother

\end{document}
